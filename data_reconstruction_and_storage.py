import numpy as np
import torch
from torch.utils.data import DataLoader
from scipy.io import savemat
from train_model_3phase import MIMOAutoEncoder, MIMODataset

# è¨­å®šåƒæ•¸ï¼ˆè«‹ä¾æ‚¨ç›®å‰æ¶æ§‹èª¿æ•´ï¼‰
encoding_dim = 32  # è«‹ä¾æ‚¨çš„å…·é«”æƒ…å½¢èª¿æ•´
model_path = 'saved_models/best_final_model_stage3.pth'
test_data_path = 'processed_H_test.npy'
global_H_max_path = 'global_H_max.npy'
output_path_mat = 'output_csi.mat'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# å°æ•¸åæ­£è¦åŒ–å‡½æ•¸
def logarithmic_denormalization(H_normalized, global_H_max):
    H_real = H_normalized[:, 0]
    H_imag = H_normalized[:, 1]

    # æ­£è¦åŒ–å¾Œæ•¸æ“šçš„å¹…å€¼
    H_magnitude_normalized = np.sqrt(H_real ** 2 + H_imag ** 2)

    # é‚„åŸè‡³åŸå§‹æ•¸æ“šçš„å¹…å€¼ (inverse operation)
    H_magnitude_original = np.power(1 + global_H_max, H_magnitude_normalized) - 1

    # ä¿ç•™ç›¸ä½è³‡è¨Š
    original_magnitude_norm = np.maximum(H_magnitude_normalized, 1e-12)
    unit_real = H_real / original_magnitude_norm
    unit_imag = H_imag / original_magnitude_norm

    # è¨ˆç®—é‚„åŸå¾Œçš„å¯¦éƒ¨èˆ‡è™›éƒ¨è³‡æ–™
    H_real_original = unit_real * H_magnitude_original
    H_imag_original = unit_imag * H_magnitude_original

    # åˆä½µæˆè¤‡æ•¸æ•¸æ“š
    return H_real_original + 1j * H_imag_original



# å­˜å„²çµæœè‡³.matèˆ‡.npyæª”
def save_original_and_reconstructed_to_mat_and_npy(original, reconstructed, output_path):
    mat_dict = {
        'original_data': original,
        'reconstructed_data': reconstructed
    }
    savemat(output_path, mat_dict)
    np.save(output_path.replace('.mat', '_original.npy'), original)
    np.save(output_path.replace('.mat', '_reconstructed.npy'), reconstructed)


# Load æ¸¬è©¦è³‡æ–™åŠglobal max
test_data = np.load(test_data_path)
global_H_max = np.load(global_H_max_path)

# Tensoræ ¼å¼è³‡æ–™è™•ç†
test_dataset = MIMODataset(test_data_path)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

print(f"æ¸¬è©¦æ•¸æ“šå½¢ç‹€: {test_dataset.data.shape}")
print(f"å…¨åŸŸæœ€å¤§å€¼ (global_H_max): {global_H_max}")

# å°å…¥æ¨¡å‹
model = MIMOAutoEncoder(encoding_dim).to(device)  # æ³¨æ„é€™è£¡æ˜¯å¦èˆ‡åŸå®šç¾©ç›¸ç¬¦
model.load_state_dict(torch.load(model_path, map_location=device))
model.eval()
print("ğŸš€ é–‹å§‹é€²è¡Œæ¸¬è©¦...")
# è¨ˆç®—æ¨¡å‹è¼¸å‡ºï¼Œä¸¦é€²è¡Œè¤‡æ•¸åˆä½µèˆ‡åæ­£è¦åŒ–ï¼ˆå·²åŠ å…¥shapeåˆ—å°ï¼‰
original_data = []
reconstructed_data = []

with torch.no_grad():
    for data_batch in test_loader:


        data = data_batch.to(device)

        print(f"data shape: {data.shape}")

        recon_data = model(data)

        print(f"recon datashape: {recon_data.shape}")



        original_denorm = logarithmic_denormalization(data.cpu().numpy(), global_H_max)
        recon_denorm = logarithmic_denormalization(recon_data.cpu().numpy(), global_H_max)



        original_data.append(original_denorm)
        reconstructed_data.append(recon_denorm)



# è½‰æ›æˆnumpyæ ¼å¼
original_data = np.concatenate(original_data, axis=0)
reconstructed_data = np.concatenate(reconstructed_data, axis=0)

print(f"âœ… åŸå§‹æ•¸æ“šå½¢ç‹€: {original_data.shape}")
print(f"âœ… é‡å»ºæ•¸æ“šå½¢ç‹€: {reconstructed_data.shape}")

# å°‡çµæœå­˜å„²ç‚ºmatå’Œnpyæª”æ¡ˆ
save_original_and_reconstructed_to_mat_and_npy(original_data, reconstructed_data, output_path_mat)

print(f"âœ… åŸå§‹æ•¸æ“šå’Œé‡å»ºæ•¸æ“šå·²å„²å­˜ç‚º: {output_path_mat}")

