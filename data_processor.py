import numpy as np
import pickle
import os

from torch.onnx.symbolic_opset9 import contiguous


def preprocess_H(H):
    """
    å°‡ H è½‰æ›ç‚ºç¥ç¶“ç¶²çµ¡å¯ç”¨çš„å¯¦æ•¸æ ¼å¼ï¼š
    å°‡å¯¦éƒ¨å’Œè™›éƒ¨åˆ†åˆ¥ä½œç‚ºå…©å€‹é€šé“ã€‚

    åŸå§‹ H å½¢ç‹€: [batch, rx, tx, carriers]
    è½‰æ›å¾Œå½¢ç‹€: [batch, 2, rx, tx, carriers]
    å…¶ä¸­ç¬¬2å€‹ç¶­åº¦çš„é€šé“0æ˜¯å¯¦éƒ¨ï¼Œé€šé“1æ˜¯è™›éƒ¨
    """
    H_real = np.real(H)  # å–å¾—å¯¦éƒ¨
    H_imag = np.imag(H)  # å–å¾—è™›éƒ¨

    # å°‡å¯¦éƒ¨è™›éƒ¨ç›´æ¥æ”¾åœ¨ç¬¬äºŒå€‹ç¶­åº¦ï¼ˆaxis=1ï¼‰
    H_combined = np.stack([H_real, H_imag], axis=-4)
    return H_combined



def normalize_H_logarithmic(H, global_H_max):
    """
    ä½¿ç”¨å°æ•¸æ­£è¦åŒ–ï¼Œå°‡ H çš„å¹…å€¼å£“ç¸®åˆ°ç¯„åœ [0, 1]ã€‚
    
    Args:
    - H: è¤‡æ•¸æ•¸æ“šçš„çŸ©é™£ï¼Œæˆ–å·²ç¶“è½‰æ›ç‚ºå¯¦æ•¸ï¼ˆå«å¯¦éƒ¨è™›éƒ¨é€šé“ï¼‰çš„çŸ©é™£ã€‚
    - global_H_max: å…¨åŸŸæœ€å¤§å€¼ï¼Œç”¨æ–¼æ­£è¦åŒ–ç¸®æ”¾ã€‚

    Returns:
    - H_normalized: æ­£è¦åŒ–å¾Œçš„çŸ©é™£æ•¸æ“šã€‚
    """
    if global_H_max > 0:
        if np.iscomplexobj(H):
            # å¦‚æœè¼¸å…¥æ˜¯è¤‡æ•¸ï¼Œå…ˆè¨ˆç®—å¹…å€¼ç„¶å¾Œå°æ•¸æ­£è¦åŒ–
            H_normalized = np.log(1 + np.abs(H)) / np.log(1 + global_H_max)
            return H_normalized
        else:
            # å¦‚æœè¼¸å…¥å·²æ˜¯å¯¦æ•¸ï¼ˆå…·æœ‰å¯¦éƒ¨è™›éƒ¨é€šé“çš„æ ¼å¼ï¼‰
            # æ³¨æ„ï¼šH å½¢ç‹€ç‚º [batch, rx, tx, carriers, 2]
            # è¨ˆç®—æ¯å€‹è¤‡æ•¸çš„å¹…å€¼
            H_real = H[:,0]
            H_imag = H[:,1]
            H_magnitude = np.sqrt(H_real ** 2 + H_imag ** 2)

            # å°å¹…å€¼é€²è¡Œå°æ•¸æ­£è¦åŒ–
            H_magnitude_normalized = np.log(1 + H_magnitude) / np.log(1 + global_H_max)

            # ä¿æŒç›¸ä½ä¸è®Šï¼Œèª¿æ•´å¹…å€¼
            # è¨ˆç®—åŸå§‹å¹…å€¼ï¼ˆé¿å…é™¤ä»¥é›¶ï¼‰
            original_magnitude = np.maximum(np.sqrt(H_real ** 2 + H_imag ** 2), 1e-12)

            # è¨ˆç®—å–®ä½å‘é‡ï¼ˆä¿æŒç›¸ä½ï¼‰
            unit_real = H_real / original_magnitude
            unit_imag = H_imag / original_magnitude

            # æ‡‰ç”¨æ–°çš„å¹…å€¼
            normalized_real = unit_real * H_magnitude_normalized
            normalized_imag = unit_imag * H_magnitude_normalized

            # åˆä½µç‚ºæœ€çµ‚è¼¸å‡º
            return np.stack([normalized_real, normalized_imag], axis=-4)
    else:
        raise ValueError("å…¨åŸŸæœ€å¤§å€¼ global_H_max å¿…é ˆå¤§æ–¼ 0")


def process_and_save_H(D="train"):
    """
    åŠ è¼‰ DeepMIMO æ•¸æ“šï¼Œè™•ç† H çŸ©é™£ï¼Œä¸¦ä¿å­˜è‡³æœ¬åœ°æ–‡ä»¶ç³»çµ±ã€‚
    - è¨“ç·´éšæ®µ (D=="train") æ™‚è¨ˆç®—ä¸¦å„²å­˜ global_H_maxã€‚
    - æ¸¬è©¦éšæ®µ (D=="test") æ™‚åŠ è¼‰å·²ä¿å­˜çš„ global_H_maxã€‚
    """
    # ç¢ºèªæ–‡ä»¶å­˜åœ¨
    if not os.path.exists(PICKLE_LOAD_PATH):
        print(f"âŒ æ‰¾ä¸åˆ° DeepMIMO æ•¸æ“šé›† `{PICKLE_LOAD_PATH}`ï¼Œè«‹å…ˆåŸ·è¡Œ `main_generate.py` ç”Ÿæˆæ•¸æ“šï¼")
        return

    # åŠ è¼‰ DeepMIMO æ•¸æ“š
    print(f"ğŸ“¥ æ­£åœ¨åŠ è¼‰ DeepMIMO æ•¸æ“šï¼š{PICKLE_LOAD_PATH}...")
    with open(PICKLE_LOAD_PATH, "rb") as file:
        dataset = pickle.load(file)

    # æå– LOS ä½¿ç”¨è€…çš„æ•¸æ“š
    LoS_status = dataset[0]['user']['LoS']
    valid_users = np.where(LoS_status == 1)[0]  # åªé¸æ“‡ LOS ç”¨æˆ¶

    if len(valid_users) == 0:
        print("âŒ æ²’æœ‰æœ‰æ•ˆçš„ LOS ä½¿ç”¨è€…ï¼Œè«‹æª¢æŸ¥æ•¸æ“šé›†ï¼")
        return

    print(f"âœ… æ‰¾åˆ° {len(valid_users)} åæœ‰æ•ˆ LOS ä½¿ç”¨è€…ï¼Œé–‹å§‹è™•ç† H çŸ©é™£...")

    # Step 1: æå–æ‰€æœ‰æœ‰æ•ˆä½¿ç”¨è€…çš„åŸå§‹è¤‡æ•¸ H çŸ©é™£
    H_complex = np.array([dataset[0]['user']['channel'][idx] for idx in valid_users])

    if D == "train":
        # è¨ˆç®— global_H_max ä¸¦å„²å­˜
        global_H_max = np.max(np.abs(H_complex))
        np.save(GLOBAL_H_MAX_SAVE_PATH, global_H_max)
        print(f"ğŸŒŸ è¨ˆç®—ä¸¦ä¿å­˜å…¨åŸŸæœ€å¤§å€¼ global_H_max: {global_H_max}")
    elif D == "test":
        # è¼‰å…¥ global_H_max
        if os.path.exists(GLOBAL_H_MAX_SAVE_PATH):
            global_H_max = np.load(GLOBAL_H_MAX_SAVE_PATH)
            print(f"ğŸ“¥ è¼‰å…¥å…¨åŸŸæœ€å¤§å€¼ global_H_max: {global_H_max}")
        else:
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°å·²ä¿å­˜çš„ global_H_max (`{GLOBAL_H_MAX_SAVE_PATH}`)ã€‚"
                                    f" è«‹å…ˆè™•ç†è¨“ç·´æ•¸æ“šä¸¦è¨ˆç®— global_H_maxï¼")
    else:
        raise ValueError("âŒ ç„¡æ•ˆçš„åƒæ•¸ Dï¼Œå¿…é ˆç‚º 'train' æˆ– 'test'ã€‚")

    # Step 3: å…ˆå°‡ H è½‰æ›æˆåˆ†é›¢å¯¦éƒ¨è™›éƒ¨çš„æ ¼å¼
    H_processed = np.array([preprocess_H(H) for H in H_complex])

    # Step 4: å° H_processed é€²è¡Œå°æ•¸æ­£è¦åŒ–
    H_all_normalized = normalize_H_logarithmic(H_processed, global_H_max)

    # Step 5: ä¿å­˜æ­£è¦åŒ–å¾Œçš„ H
    np.save(NORM_H_SAVE_PATH, H_all_normalized)
    print(f"âœ… è™•ç†å¾Œçš„ H çŸ©é™£å·²å„²å­˜è‡³ `{NORM_H_SAVE_PATH}`ï¼Œå…¶å½¢ç‹€ç‚º: {H_all_normalized.shape}")

    # é¡¯ç¤ºéƒ¨åˆ†è™•ç†å¾Œçš„æ•¸æ“šé€²è¡Œé©—è­‰
    print(f"ğŸ“Š ç¤ºä¾‹è™•ç†æ•¸æ“šï¼ˆç¬¬ä¸€å€‹æ¨£æœ¬çš„å°ºå¯¸ï¼‰: {H_all_normalized[0].shape}")
    print(f"ğŸ“Š ç¤ºä¾‹æ•¸æ“š - å¯¦éƒ¨é€šé“å‰å¹¾å€‹å€¼: {H_all_normalized[0,0].flatten()[:5]}")
    print(f"ğŸ“Š ç¤ºä¾‹æ•¸æ“š - è™›éƒ¨é€šé“å‰å¹¾å€‹å€¼: {H_all_normalized[0,1].flatten()[:5]}")



if __name__ == "__main__":
    # è¨­å®šå­˜æª”è·¯å¾‘
    PICKLE_LOAD_PATH = "DeepMIMO_trainset.pkl"  # è®€å– DeepMIMO ç”Ÿæˆçš„æ•¸æ“š
    NORM_H_SAVE_PATH = "processed_H_train.npy"  # å„²å­˜è™•ç†å¾Œçš„ H
    GLOBAL_H_MAX_SAVE_PATH = "global_H_max.npy"  # å„²å­˜å…¨åŸŸæœ€å¤§å¹…å€¼

    process_and_save_H("train")
    H_normalized = np.load(NORM_H_SAVE_PATH)


    # é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯ - æ³¨æ„ç¾åœ¨æˆ‘å€‘æœ‰å…©å€‹é€šé“
    print(f"H_normalizedå¯¦éƒ¨ç¯„åœ: min={np.min(H_normalized[:,0])}, max={np.max(H_normalized[:,0])}")
    print(f"H_normalizedè™›éƒ¨ç¯„åœ: min={np.min(H_normalized[:,1])}, max={np.max(H_normalized[:,1])}")

    # è¨ˆç®—å¹…å€¼ä»¥æª¢æŸ¥æ­£è¦åŒ–æ•ˆæœ
    H_magnitude = np.sqrt(H_normalized[:,0] ** 2 + H_normalized[:,1] ** 2)
    print(f"H_normalizedå¹…å€¼ç¯„åœ: min={np.min(H_magnitude)}, max={np.max(H_magnitude)}")

