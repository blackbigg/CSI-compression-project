import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt
import os

import random

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    # CUDA deterministic è¨­å®šï¼Œä½¿ GPU è¨“ç·´çµæœå¯ç©©å®šé‡ç¾
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Dataset
class MIMODataset(Dataset):
    def __init__(self, data_path):
        self.data = np.load(data_path)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

# è¤‡æ•¸æ“ä½œ
def custom_abs(data):
    real, imag = data[:, 0], data[:, 1]
    return torch.sqrt(real ** 2 + imag ** 2 + 1e-8)  # åŠ å…¥epsiloné˜²æ­¢sqrt(0)


def custom_angle(data):
    real, imag = data[:, 0], data[:, 1]
    return torch.atan2(imag, real)


def to_complex(data):
    return torch.complex(data[:, 0], data[:, 1])


# Loss Functions (åŠ å…¥MSE Loss)

def spectral_loss(y_true, y_pred):
    y_true_abs, y_pred_abs = custom_abs(y_true), custom_abs(y_pred)
    abs_loss = F.mse_loss(y_pred_abs, y_true_abs)

    y_true_angle, y_pred_angle = custom_angle(y_true), custom_angle(y_pred)
    angle_diff = (y_true_angle - y_pred_angle + torch.pi) % (2 * torch.pi) - torch.pi
    angle_loss = torch.mean(angle_diff ** 2)

    return abs_loss + 0.5* angle_loss + 1e-8


def nmse_loss(y_true, y_pred):
    y_true_complex, y_pred_complex = to_complex(y_true), to_complex(y_pred)
    error_power = torch.mean(torch.abs(y_true_complex - y_pred_complex) ** 2)
    signal_power = torch.mean(torch.abs(y_true_complex) ** 2) + 1e-8
    return error_power / signal_power


def mse_loss(y_true, y_pred):
    y_true_complex, y_pred_complex = to_complex(y_true), to_complex(y_pred)
    return torch.mean(torch.abs(y_true_complex - y_pred_complex) ** 2)


# æ¨¡å‹å®šç¾©ï¼ˆæ”¹é€²ç‰ˆï¼‰
class SEAttention(nn.Module):
    def __init__(self, channel, reduction=8):
        super(SEAttention, self).__init__()

        reduced_channels = max(channel // reduction, 1)  # ä¿è­‰ä¸æœƒè®Šæˆ0

        self.avg_pool = nn.AdaptiveAvgPool3d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, reduced_channels, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(reduced_channels, channel, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1, 1)
        return x * y.expand_as(x)



class ResidualBlock(nn.Module):
    def __init__(self, img_channels=2):
        super(ResidualBlock, self).__init__()

        self.conv = nn.Sequential(nn.Conv3d(img_channels, 8, kernel_size=3, padding=1),
                                  nn.BatchNorm3d(8, eps=1e-03, momentum=0.99),
                                  nn.LeakyReLU(negative_slope=0.3),
                                  nn.Conv3d(8, 16, kernel_size=3, padding=1),
                                  nn.BatchNorm3d(16, eps=1e-03, momentum=0.99),
                                  nn.LeakyReLU(negative_slope=0.3),
                                  nn.Conv3d(16, img_channels, kernel_size=3, padding=1),
                                  nn.BatchNorm3d(img_channels, eps=1e-03, momentum=0.99))

        self.leakyRelu = nn.LeakyReLU(negative_slope=0.3)

    def forward(self, x):
        ori_x = x

        # concatenate
        x = self.conv(x) + ori_x

        return self.leakyRelu(x)


class MIMOAutoEncoder(nn.Module):
    def __init__(self, encoding_dim=32, dropout_rate=0.0):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv3d(2, 16, kernel_size=3, stride=1, padding=1),
            nn.LeakyReLU(0.01), nn.BatchNorm3d(16),
            nn.Dropout3d(dropout_rate),
            SEAttention(channel=16),  # <-- åŠ å…¥æ³¨æ„åŠ›æ¨¡å¡Š


            nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1),
            nn.LeakyReLU(0.01), nn.BatchNorm3d(32),
            nn.Dropout3d(dropout_rate),
            SEAttention(channel=32),  # <-- åŠ å…¥æ³¨æ„åŠ›æ¨¡å¡Š

            nn.Conv3d(32, 2, kernel_size=3, stride=1, padding=1),
            nn.LeakyReLU(0.01), nn.BatchNorm3d(2),
            nn.Dropout3d(dropout_rate),
            SEAttention(channel=2),  # <-- åŠ å…¥æ³¨æ„åŠ›æ¨¡å¡Š



            nn.Flatten(),
            nn.Tanh(),
            nn.Dropout(dropout_rate),
            nn.Linear(2 * 32 * 2 * 32, encoding_dim),
        )

        self.decoder = nn.Sequential(
            nn.Linear(encoding_dim, 2 * 32 * 2 * 32),
            nn.Unflatten(1, (2, 32, 2, 32)),



            nn.ConvTranspose3d(2, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm3d(32),
            nn.LeakyReLU(0.01),
            ResidualBlock(32),

            nn.ConvTranspose3d(32, 16, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm3d(16),
            nn.LeakyReLU(0.01),
            ResidualBlock(16),

            nn.ConvTranspose3d(16, 2, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm3d(2),
            nn.LeakyReLU(0.01),
            ResidualBlock(2),

            nn.Conv3d(2, 2, kernel_size=3, padding=1),
            nn.Tanh(),
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded




# èª¿æ•´ç‰ˆçš„è¨“ç·´å‡½å¼ï¼šæ”¯æ´ä¸åŒ loss function çµ„åˆ
def train_model_curriculum(model, train_loader, val_loader, device, stage, epochs, patience):
    model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, "min", patience=5, factor=0.5)

    best_val_loss = float("inf")
    counter = 0  # early stopping è¨ˆæ•¸å™¨

    train_losses = []
    val_losses = []

    print(f"ğŸ”” é–‹å§‹è¨“ç·´éšæ®µ {stage} ...")

    for epoch in range(epochs):
        model.train()
        epoch_train_loss = 0.0
        current_lr = optimizer.param_groups[0]['lr']

        for batch in train_loader:
            batch = batch.to(device)
            optimizer.zero_grad()
            output = model(batch)

            # æ ¹æ“šéšæ®µåˆ‡æ› loss function çµ„åˆ
            if stage == 1:
                loss = mse_loss(batch, output)
            elif stage == 2:
                loss = mse_loss(batch, output) + nmse_loss(batch, output)
            elif stage == 3:
                loss = mse_loss(batch, output) + nmse_loss(batch, output) + spectral_loss(batch, output)
            else:
                raise ValueError("æœªçŸ¥çš„éšæ®µï¼")


            if torch.isnan(loss):
                print(f"Epoch {epoch + 1}: Loss ç‚º NaNï¼Œåœæ­¢è¨“ç·´ï¼")
                return train_losses, val_losses

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            epoch_train_loss += loss.item()

        avg_train_loss = epoch_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        # é©—è­‰éšæ®µ
        model.eval()
        epoch_val_loss = 0.0
        with torch.no_grad():
            for batch in val_loader:
                batch = batch.to(device)
                output = model(batch)
                if stage == 1:
                    loss = mse_loss(batch, output)
                elif stage == 2:
                    loss = mse_loss(batch, output) + nmse_loss(batch, output)
                elif stage == 3:
                    loss = mse_loss(batch, output) + nmse_loss(batch, output) + spectral_loss(batch, output)
                epoch_val_loss += loss.item()

        avg_val_loss = epoch_val_loss / len(val_loader)
        val_losses.append(avg_val_loss)

        print(f"[Stage {stage}] Epoch [{epoch+1}/{epochs}], Training Loss: {avg_train_loss:.6f}, Validation Loss: {avg_val_loss:.6f}, LR: {current_lr:.6f}")
        scheduler.step(avg_val_loss)

        # Early Stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            counter = 0
            torch.save(model.state_dict(), f"saved_models/mimo_autoencoder_stage{stage}.pth")
        else:
            counter += 1
            if counter >= patience:
                print(f"âœ… Early stopping at epoch {epoch+1} in stage {stage}")
                break
    # â­ åŠ åœ¨è¨“ç·´è¿´åœˆä¹‹å¾Œï¼Œç¢ºä¿æœ€ä½³æ¬Šé‡
    model.load_state_dict(torch.load(f"saved_models/mimo_autoencoder_stage{stage}.pth"))
    # å†æ¬¡å„²å­˜ç¢ºä¿æœ€çµ‚æœ€ä½³æ¨¡å‹å®‰å…¨
    torch.save(model.state_dict(), f"saved_models/best_final_model_stage{stage}.pth")

    return train_losses, val_losses

def visualize_reconstruction(model, dataset, device, num_samples=5):
    model.eval()
    plt.figure(figsize=(18, 6))

    for i in range(num_samples):
        # åŠ å…¥batchç¶­åº¦ä¸¦ç§»è‡³è£ç½®
        original = torch.tensor(dataset[i]).unsqueeze(0).to(device)

        # æ¨¡å‹é‡å»ºè³‡æ–™
        with torch.no_grad():
            reconstructed = model(original)

        # æ­¤è™•å°‡è¤‡æ•¸è½‰ç‚ºå–®ç´”çš„å¹…å€¼ï¼ˆå–çµ•å°å€¼ï¼‰
        original_csi = np.abs(to_complex(original.cpu()).numpy().flatten())
        reconstructed_csi = np.abs(to_complex(reconstructed.cpu()).numpy().flatten())

        # åŸå§‹CSIè³‡æ–™ç¹ªè£½
        plt.subplot(2, num_samples, i + 1)
        plt.plot(original_csi, label='Original')
        plt.title(f'Original CSI {i + 1}')
        plt.xlabel('Subcarrier Index')
        plt.ylabel('Amplitude')
        plt.grid(True)

        # é‡å»ºCSIè³‡æ–™ç¹ªè£½
        plt.subplot(2, num_samples, i + 1 + num_samples)
        plt.plot(reconstructed_csi, label='Reconstructed', color='r')
        plt.title(f'Reconstructed CSI {i + 1}')
        plt.xlabel('Subcarrier Index')
        plt.ylabel('Amplitude')
        plt.grid(True)

    plt.tight_layout()
    plt.savefig('reconstruction_comparison.png')
    plt.show()

# ğŸŒŸ ä¸»ç¨‹å¼æ”¹ç‚ºä¸‰éšæ®µè¨“ç·´
def main():
    set_seed(42)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è£ç½®ï¼š{device}")

    # å»ºç«‹å„²å­˜æ¨¡å‹è³‡æ–™å¤¾
    if not os.path.exists("saved_models"):
        os.mkdir("saved_models")
    # æª¢æŸ¥è³‡æ–™å®Œæ•´æ€§
    data_np = np.load("processed_H_train.npy")
    assert not np.isnan(data_np).any(), "è³‡æ–™å«æœ‰ NaN!"
    assert not np.isinf(data_np).any(), "è³‡æ–™å«æœ‰ Inf!"
    print('è³‡æ–™æœ€å¤§å€¼:', data_np.max(), 'è³‡æ–™æœ€å°å€¼:', data_np.min())
    # è®€å– dataset
    dataset = MIMODataset("processed_H_train.npy")
    # **æ–°å¢ Validation Set**
    train_size = int(0.8 * len(dataset))  # 80% çµ¦ Training
    val_size = len(dataset) - train_size  # 20% çµ¦ Validation
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

    model = MIMOAutoEncoder().to(device)
    print(f"æ¨¡å‹ç¸½åƒæ•¸æ•¸é‡: {count_parameters(model):,}")

    all_train_losses = []
    all_val_losses = []

    for stage in range(1, 4):
        if stage > 1:
            model.load_state_dict(
                torch.load(f"saved_models/mimo_autoencoder_stage{stage - 1}.pth", map_location=device))

        train_losses, val_losses = train_model_curriculum(
            model, train_loader, val_loader, device,
            stage=stage, epochs=50, patience=10
        )

        all_train_losses += train_losses
        all_val_losses += val_losses

    plt.figure()
    plt.plot(all_train_losses, label="Training Loss")
    plt.plot(all_val_losses, label="Validation Loss", linestyle="--")
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title("Loss vs Epoch (3-stage Curriculum Learning)")
    plt.legend()
    plt.grid(True)
    plt.savefig('training_validation_loss_curriculum.png')
    plt.show()

    visualize_reconstruction(model, dataset, device, num_samples=5)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


if __name__ == "__main__":
    main()

